{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUonhhHvWnhcGgvqb+EmyI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdmartinev/ArtificialIntelligenceIM/blob/main/Lecture06/notebooks/L06_VAE_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Autoencoders Variacionales (VAE)](https://arxiv.org/abs/1906.02691)\n",
        "\n",
        "Los autoencoders tradicionales comprimen los datos de entrada en un espacio latente de menor dimensión, capturando efectivamente las características más importantes de las imágenes de entrada. Sin embargo, los vectores latentes producidos por los autoencoders tradicionales a menudo carecen de interpretabilidad. Por ejemplo, si consideramos el conjunto de datos MNIST, puede ser difícil entender qué dígitos corresponden a vectores latentes específicos porque los vectores latentes cercanos podrían no representar dígitos similares.\n",
        "\n",
        "En contraste, cuando se entrenan modelos generativos, es ventajoso tener un espacio latente bien estructurado e interpretable. Aquí es donde entran en juego los **Autoencoders Variacionales (VAE)**.\n",
        "\n",
        "Un VAE es un tipo de autoencoder que aprende a predecir una *distribución estadística* sobre los parámetros latentes, conocida como la **distribución latente**. Por ejemplo, en un VAE, podríamos suponer que los vectores latentes siguen una distribución Gaussiana\n",
        "\n",
        "$$\n",
        "N(\\mu_{\\mathbf{z}}, e^{\\log \\sigma_{\\mathbf{z}}})\n",
        "$$\n",
        "\n",
        "donde $\\mu_{\\mathbf{z}}$ y $\\log \\sigma_{\\mathbf{z}} \\in \\mathbb{R}^d$. El codificador en un VAE aprende a predecir estos parámetros de la distribución, y el decodificador reconstruye la entrada muestreando un vector de esta distribución.\n",
        "\n",
        "Para resumir el proceso de un VAE:\n",
        "\n",
        "1. A partir de los datos de entrada, el codificador predice $\\mu_{\\mathbf{z}}$ y $\\log \\sigma_{\\mathbf{z}}$ (predecimos el logaritmo de la desviación estándar en lugar de la desviación estándar en sí por estabilidad numérica).\n",
        "2. Luego, muestreamos un vector latente $\\mathbf{z}_{\\text{sample}}$ de la distribución\n",
        "\n",
        "$$\n",
        "N(\\mu_{\\mathbf{z}}, e^{\\log \\sigma_{\\mathbf{z}}})\n",
        "$$\n",
        "\n",
        "3. El decodificador intenta reconstruir la entrada original utilizando $\\mathbf{z}_\\text{sample}}$ como entrada para la red del decodificador.\n",
        "\n",
        "Este proceso permite que el VAE aprenda un espacio latente suave y continuo donde la interpolación entre puntos en el espacio latente resulta en variaciones significativas en la salida. Esta propiedad hace que los VAE sean particularmente poderosos para generar nuevos datos que son similares a los datos de entrenamiento.\n",
        "\n",
        "Aquí tienes una representación visual de la arquitectura del VAE:\n",
        "\n",
        "![Imagen de la arquitectura del VAE](https://drive.google.com/uc?id=1x30qjCzLYIuJJR_dBFDHGolpltYLCCP2)\n",
        "\n",
        "> Imagen tomada de [este blog](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) de Isaak Dykeman\n"
      ],
      "metadata": {
        "id": "8N9rYBMwepwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "cO1dVyOHg81c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "train_size = 0.9\n",
        "lr = 1e-3\n",
        "eps = 1e-8\n",
        "batch_size = 256\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "zDmdZTRghlsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist(train_part, transform=None):\n",
        "    dataset = torchvision.datasets.MNIST('.', download=True, transform=transform)\n",
        "    train_part = int(train_part * len(dataset))\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_part, len(dataset) - train_part])\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "SElFv3NKho-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset, test_dataset = mnist(train_size, transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, drop_last=True, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "dataloaders = (train_dataloader, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGlwpd8Ph-bB",
        "outputId": "4b7dd65b-9679-4d3f-c6d7-5b037d7622e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 25451390.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1714588.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 14663962.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4063679.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotn(n, data, noisy=False, super_res=None):\n",
        "    fig, ax = plt.subplots(1, n)\n",
        "    for i, z in enumerate(data):\n",
        "        if i == n:\n",
        "            break\n",
        "        preprocess = z[0].reshape(1, 28, 28) if z[0].shape[1] == 28 else z[0].reshape(1, 14, 14) if z[0].shape[1] == 14 else z[0]\n",
        "        if super_res is not None:\n",
        "            _transform = transforms.Resize((int(preprocess.shape[1] / super_res), int(preprocess.shape[2] / super_res)))\n",
        "            preprocess = _transform(preprocess)\n",
        "\n",
        "        if noisy:\n",
        "            shapes = list(preprocess.shape)\n",
        "            preprocess += noisify(shapes)\n",
        "\n",
        "        ax[i].imshow(preprocess[0])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YHQLxrqFhrVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noisify(shapes):\n",
        "    return np.random.normal(loc=0.5, scale=0.3, size=shapes)"
      ],
      "metadata": {
        "id": "DF517htBhtMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wILO-CVtbu8P"
      },
      "outputs": [],
      "source": [
        "class VAEEncoder(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.intermediate_dim = 512\n",
        "        self.latent_dim = 2\n",
        "        self.linear = nn.Linear(784, self.intermediate_dim)\n",
        "        self.z_mean = nn.Linear(self.intermediate_dim, self.latent_dim)\n",
        "        self.z_log = nn.Linear(self.intermediate_dim, self.latent_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input):\n",
        "        bs = input.shape[0]\n",
        "\n",
        "        hidden = self.relu(self.linear(input))\n",
        "        z_mean = self.z_mean(hidden)\n",
        "        z_log = self.z_log(hidden)\n",
        "\n",
        "        eps = torch.FloatTensor(np.random.normal(size=(bs, self.latent_dim))).to(device)\n",
        "        z_val = z_mean + torch.exp(z_log) * eps\n",
        "        return z_mean, z_log, z_val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAEDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.intermediate_dim = 512\n",
        "        self.latent_dim = 2\n",
        "        self.linear = nn.Linear(self.latent_dim, self.intermediate_dim)\n",
        "        self.output = nn.Linear(self.intermediate_dim, 784)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden = self.relu(self.linear(input))\n",
        "        decoded = self.sigmoid(self.output(hidden))\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "gnB3VFDGg21O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAEAutoEncoder(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.encoder = VAEEncoder(device)\n",
        "        self.decoder = VAEDecoder()\n",
        "        self.z_vals = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        bs, c, h, w = input.shape[0], input.shape[1], input.shape[2], input.shape[3]\n",
        "        input = input.view(bs, -1)\n",
        "        encoded = self.encoder(input)\n",
        "        self.z_vals = encoded\n",
        "        decoded = self.decoder(encoded[2])\n",
        "        return decoded\n",
        "\n",
        "    def get_zvals(self):\n",
        "        return self.z_vals"
      ],
      "metadata": {
        "id": "nD0uGxXbhFp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los autoencoders variacionales utilizan una función de pérdida compleja que consta de dos partes:\n",
        "\n",
        "* **Pérdida de reconstrucción**: es la función de pérdida que muestra qué tan cercana es la imagen reconstruida a la original (puede ser MSE). Es la misma función de pérdida que se utiliza en los autoencoders normales.\n",
        "* **Pérdida KL**, que asegura que las distribuciones de las variables latentes se mantengan cercanas a una distribución normal. Se basa en la noción de la [divergencia de Kullback-Leibler](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained), una métrica para estimar cuán similares son dos distribuciones estadísticas.\n"
      ],
      "metadata": {
        "id": "y3pLVFoIhW6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(preds, targets, z_vals):\n",
        "    mse = nn.MSELoss()\n",
        "    reconstruction_loss = mse(preds, targets.view(targets.shape[0], -1)) * 784.0\n",
        "    temp = 1.0 + z_vals[1] - torch.square(z_vals[0]) - torch.exp(z_vals[1])\n",
        "    kl_loss = -0.5 * torch.sum(temp, axis=-1)\n",
        "    return torch.mean(reconstruction_loss + kl_loss)"
      ],
      "metadata": {
        "id": "yifee2DwhZvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAEAutoEncoder(device).to(device)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=lr, eps=eps)"
      ],
      "metadata": {
        "id": "1LLJlYl5hfUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(dataloaders, model, optimizer, epochs, device):\n",
        "    tqdm_iter = tqdm(range(epochs))\n",
        "    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n",
        "\n",
        "    for epoch in tqdm_iter:\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            imgs, labels = batch\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            preds = model(imgs)\n",
        "            z_vals = model.get_zvals()\n",
        "            loss = vae_loss(preds, imgs, z_vals)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                imgs, labels = batch\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                preds = model(imgs)\n",
        "                z_vals = model.get_zvals()\n",
        "                loss = vae_loss(preds, imgs, z_vals)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_dataloader)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n",
        "        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n",
        "        tqdm_iter.refresh()"
      ],
      "metadata": {
        "id": "xGqt0NMVi2tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vae(dataloaders, model, optimizer, epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL4Vw2npi6tr",
        "outputId": "15dace0a-38cb-412e-acd9-ea1f47863f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [06:18<00:00, 12.62s/it, train loss:=35.2, test loss:=35.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "plots = 5\n",
        "for i, data in enumerate(test_dataset):\n",
        "    if i == plots:\n",
        "        break\n",
        "    predictions.append(model(data[0].to(device).unsqueeze(0)).view(1, 28, 28).detach().cpu())\n",
        "plotn(plots, test_dataset)\n",
        "plotn(plots, predictions)"
      ],
      "metadata": {
        "id": "31C7zkYwjBRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Tarea**: En nuestro ejemplo, hemos entrenado un VAE completamente conectado (fully-connected). Ahora toma la CNN del autoencoder tradicional mencionado anteriormente y crea un VAE basado en CNN.\n"
      ],
      "metadata": {
        "id": "fLlUZpzejGWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Materiales Adicionales\n",
        "\n",
        "* [Explicación VAE](https://mbernste.github.io/posts/vae/)\n"
      ],
      "metadata": {
        "id": "e51R1virjoOb"
      }
    }
  ]
}