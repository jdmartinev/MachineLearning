{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO49Pj8EabyhslGnqo8zRy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdmartinev/ArtificialIntelligenceIM/blob/main/Lecture06/notebooks/L06_ConvAE_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional autoencoder\n",
        "\n",
        "\n",
        "Let's create simplest autoencoder for MNIST!"
      ],
      "metadata": {
        "id": "k108t8-PlZXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es476MNKlBMm",
        "outputId": "0fc3d87d-31d8-4861-ef29-16a249211903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy,mse\n",
        "\n",
        "(x_train, y_trainclass), (x_test, y_testclass) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotn(n,x):\n",
        "  fig,ax = plt.subplots(1,n)\n",
        "  for i,z in enumerate(x[0:n]):\n",
        "    ax[i].imshow(z.reshape(28,28) if z.size==28*28 else z.reshape(14,14) if z.size==14*14 else z)\n",
        "  plt.show()\n",
        "\n",
        "plotn(5,x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "cHdWLvlplfYx",
        "outputId": "9d579693-9090-4bf3-f062-e54a38139939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACFCAYAAAD7P5rdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAejklEQVR4nO3de1xUdd4H8M9wG0C5iC4gAUoqkVpRiIqal5bVtW3LorLafdb1aTMVfFLyqbXr5upD17U0umybYrtP676sVcunbBMNs1CDpM0bmfdUxisXUa7ze/5Af78zNoIDZ86cGT7v14vX6ztnzhl+zJcz/PhdLUIIASIiIiKD+Hm6AERERNS5sPJBREREhmLlg4iIiAzFygcREREZipUPIiIiMhQrH0RERGQoVj6IiIjIUKx8EBERkaFY+SAiIiJDsfJBREREhnJb5SM/Px+9e/dGcHAwhgwZgi1btrjrW5ELmBfzYm7Mi7kxJ+bFiwk3WLZsmQgKChKLFy8W27dvFw888ICIjIwUNpvNHd+OLhPzYl7MjXkxN+bEvHg3ixD6byw3ZMgQpKen49VXXwUA2O12JCQkYMaMGfj973/f6rV2ux1HjhxBWFgYLBaL3kXrtIQQGD16NIYNG4b8/HwAruXlwvnMjb6EEKipqUFWVla775kL5zM3+tIjN8yLe/DzzJwu3DNxcXHw82u9YyVA72/e0NCA0tJSzJkzRx7z8/NDZmYmiouLf3R+fX096uvr5ePDhw+jf//+eheLzsvOzpZxa3kBmBsj+fv7X/Y9AzA3RnIlN8yLsfh5Zk6HDh1CfHx8q+foXvk4ceIEmpubERMT43A8JiYGu3bt+tH5eXl5eOaZZ350fARuRgAC9S5ep1WLGnyFdejVq5fD8UvlBWBujNCERmzERy7dMwBzY4T25IZ5MQY/z8zpwj0TFhbW5rm6Vz5cNWfOHOTm5srH1dXVSEhIQAACEWDhL4ReAkRLql1pXmRuDNDOTk/mxgDtyA3zYgx+npnU+XvmcvKie+WjR48e8Pf3h81mczhus9kQGxv7o/OtViusVqvexaCLBKLlPT527JjD8UvlBWBujOTKPQMwN0bi55n58PPM++k+1TYoKAhpaWkoLCyUx+x2OwoLC5GRkaH3t6PL5Hc+1UVFRfIY82IeqampvGdMirkxH36eeT+3dLvk5uZi0qRJGDRoEAYPHoyXX34ZtbW1mDx5sju+Hblg6dKlGDZsGPNiMtnZ2Zg2bRrvGRNibsyLn2feyy2Vj4kTJ+L48eN46qmnUFFRgdTUVKxZs+ZHg7bIePPmzWNeTCgrKwu1tbXMjQkxN+bFzzPv5ZZ1PjqiuroaERERGI3bOAhIR02iEZ9hFaqqqhAeHt6u12Bu9KdHXgDmxh14z5gXc2NOruSFe7sQERGRoVj5ICIiIkOx8kFERESG8vgiY0RGaLopTcZHp6sllr/JWCrj64onyTguP0jG/uu/dnPpiIg6F7Z8EBERkaFY+SAiIiJDsdvlMlkC1Fvl/5MebZ5fPru3jJtD7TLu1UctBxw6Xa1/X/En1cz/9aB/OLzWieZaGQ9Z/rCM++ZuarMcnZl91PUyXrj4VRn3DVS5tGvO35qxRMblg5pl/N+9h7qngNRhtXcOkfFzz7/u8Nwf7/6NjEXJNsPK1JnseUGtJrrzPnWPBVr8ZTxy+hSHa0JWbnF/wcj02PJBREREhmLlg4iIiAzVabtd/K/uJ2NhVavbHRkVKeNzQ1V3R1SEij+/zrFbxBUfnw2T8XOv/lzGm695V8b7Gs85XPOs7WcyjvvcVAvSmk7j2EEyfuS1v8o4OVB1a9k1nS17GxtlXGVXO15er9n8sn58uoxD1n/r8P3sdXUdK7AHnbttsIq7q2byqMXFnihOuxwbpP5/+uP+X3qwJJ1HxaxhMv5s4vMybhRBzk6X26wTabHlg4iIiAzFygcREREZqtN0uzSPvsHh8Z8K8mWsbZJ3h0ahZk48tei3Mg6oVe2RGctzZBx2uMnheusJ1Q0TWrLZDSX0Pv6aTYtqR6bIeNYC1X01JuSM5grn9eyC06oJufA1NXL/iz8slPGnf3lDxv3/pvIEAFc+6j1dFBc7MlK9J6F9KtUTi40vi0v8VBeRSFT3xk+jdzmcVmgZBtLfmQTVbRnl597Pzs6oYZzqOj7wK/VeT7uhSMYzu33n9Npr/jJDxqFH1d+XymFqYcVe/6vu+6BPSjpW2A5gywcREREZipUPIiIiMhQrH0RERGSoTjPmw1p+xOFxaV2CjJMDbe1+3YePqtUv955RK58W9HlPxlV21fcWs/BLl78HZ6r92A/vXCHjr9LzWzmzdXOjv5Lxmq5qjMDk/WNlvLT3WhmH9z/Z7u9lNs/cslzGz+0c28qZ5uLfp5eMd41SA1RSt/za4by4rxynRVP7nblLrST7/u2vaJ5RqzS/UanGXq29W41b6HJgu8Nr2UEXOz5VjTdb9Ij6PBtkVeMF/TRtBZP2Z8r4+oiDMv7md9rcKNprh0XdK+OoT9pZYB2w5YOIiIgMxcoHERERGarTdLs0Ha1weLzoubtkPP/navVS/393lfE30xc5fa15J66V8feZoTJurjwq4/sypst4/3+pa5PwjQulJq2mm9Jk/PdUtYmVH5xP95t84KcyLll7tYy/vV9du/5csIyjS9S0ze9PqybkwP9Zr76XamX2eoGWprZPMqGAv5x1evzcnnCnx6l96m5RK+A+nae6t5IDnd8ES99SKzbH7nC9e7kzsGiWdajLvE7G7895QcZxAWp55fsPqNWtD7x4lYy7/F+ZjNeHJsq4aEWyes1+HzgtQ3VZdxlHXW7B3cDllo8NGzbgl7/8JeLi4mCxWLBy5UqH54UQeOqpp9CzZ0+EhIQgMzMTu3fv1qu8dAmnxXGUiS+wQazGWvEejonDDs+L8yNHkpOTmReDtZUbAJg/fz7vGYPxnjEv5sb3uVz5qK2txXXXXYf8fOeD/J5//nksXLgQb7zxBjZv3owuXbpg3LhxqPPiPTC8QTOa0BURSMH1Tp8/hO8BAAsWLGBeDNZWbgDgzTff5D1jMN4z5sXc+D6Xu13Gjx+P8ePHO31OCIGXX34ZTzzxBG677TYAwDvvvIOYmBisXLkS99xzT8dKq6OoJWplyp98qJqhmk+ekvGAgf8p4+0jVbPjB38eJePoSufNi5Zi1b2SZMAimD0sPdEDPVseXDQ9RgiBH7AHAPCLX/wC4eHhps3Lxeyj1IfPwsWqu6RvoPrV1W4Ud+uu22Xsf6fqTov8hXpT+v9VrVKanH9Ixn6Htsq42+eqDI3z1Yjz9691XP7zP8eoPjX/9V87/Rlazc35A7NnzzbknrGPSJXxjcEbdX1to/Tu4nzGUcLaZqfHL8VX7xm9HP21+kM+JkT7R12tMKuddRH7in5dLb6am6M5ahbQltnamSmqq+Wu79UGiU1ZauPL0BNqdWvtW3JkiuqO3tzP+WwX7Yamfd9Un3me7HjVdcDpvn37UFFRgcxM9QsZERGBIUOGoLjY+V/g+vp6VFdXO3yRvs6hFg2odzjWVl4A5sYIdWgZvzB69Gh5jLnxPN4z5sXc+AZdKx8VFS2DOmNiYhyOx8TEyOculpeXh4iICPmVkJDg9DxqvwY4b4psLS8Ac2OECx+i0dHRDseZG8/iPWNezI1v8Phslzlz5iA3N1c+rq6uNvyXovmE82bcxmrnsygG/GqHjI+/rpogYXet2dfsPJUbS9oAGZ/IVTNQtBsAlmr+8Vl3pr+MTy5T5et+Wv0XFPG3TSrWfC9Xmx1j/K0Oj0/OVDMvotdffLb7tDc3B24JkXG0f2grZ5pLQG81ov/OKOej+EP2nXZ47Im70QyfZx0REK8W79t+4xIZazfH3Kl6AnDwT2p2RReYe9NLT+Vm9yK1QFv5HWoGpXaxtas/nSrjlNn7ZXypv01aU6etavOcefMnybjbIXNshqlr5SM2NhYAYLPZ0LNnT3ncZrMhNTXV6TVWqxVWq9Xpc6SPIAQ7Pd5aXgDmxghB5/t6jx07huRk9UHO3HgW7xnzYm58g67dLklJSYiNjUVhYaE8Vl1djc2bNyMjI6OVK8mdQtBF/pG7gHkxh2C0tD4UFantspkbz+M9Y17MjW9wueXjzJkz+P777+Xjffv2oaysDFFRUUhMTMTMmTMxb9489OvXD0lJSXjyyScRFxeHCRMm6FluQ1z96HcynnyNWrBqSS9VuRp1V7aMw/6hmvaN1iSacA5n5ONzqEWNqEQgghBsCUW86IO92IGPPvoIAwYMMFVe/EIdm/+bnlcDwTal/FPG+5oaZJz72MMy7va52tsgussxGRvR7D645wEZ77/EOa3lJgCBAIAXXngB11xzjdvvmYC+NU6P1+2K1P176enQy11kPNyqGqzfro5XJ1W6NoDQm+8ZPfkPUItXDXp3W5vnT/ynmuHV5333fOZ5c272vDTU4XH5HWpZiiq7Gq9y1677ZHzVDPW3prnG+T3q10XdAyfvVAtd3tZVLVDmB9WtmrJc/W3qW2COrhYtlysfJSUlGDNmjHx8oQ9t0qRJKCgowCOPPILa2lpMmTIFlZWVGDFiBNasWYPgYOdNZaSPapzC19ggH+/GvwEAPdELA5COBPTFXuzAQw89hKqqKubFQK3l5iqkAgAefPBB3jMG4z1jXsyN73O58jF69GgIcel9Vi0WC+bOnYu5c+d2qGDkmihLNDJx5yWft5zffXL37t0ID+cy1EZqLTdNomX03uOPP47nnnvOyGJ1erxnzIu58X0en+1iZs2VVTI+OU3tDXLwAzUD4/fz3pHxnLvVAldiq5pTkTBf0+TVSsWtszo3aoDD409SXnN63u8emiXjsJWqudc7dygxl+gSz2107t9DLfJny1KDbqPu/kHGRclva65Q/92+nj9BxtE27ifSHgduVe//e923ap5RM/nu26MWvkp+do+MfWt+X/v5x6ip8ktvd/z80i6CqO1qCfrZAc05zvmlqpl8AxfvlPG8mIWas9T4l+FlaoG1q/6gzjdjnrirLRERERmKlQ8iIiIyFLtdLpP9G9WEdc8z/y3j/336RRmXDVVdMNAMeB7QRe0l0u+tozJu2rtf30J6qWv/WObw2E9TJ558QM0yClm5xagiOQi0qObnxot6zfwtvtGNdi5KveddWjlPy36j2ndH+Ktt1g9lqmbghji1IpVfkGr8/deNarEl7Q7tFc3q2if3qm7MU3bVMB3qp14nZrOaGeAbmTDGqclqSuqKqS9ongmU0dRDag+rxkkqL83HD4IcWYLV+zPIeulOjpD/UgslWnqpBc52T1WztsZmqj2iZkX/WcaJAWomi7abplnTlW/5Rw91vNLcu/yy5YOIiIgMxcoHERERGYrdLu0QtVjNXskpVwu5hD+rRuf//cpPZLz9N2or+JSE38n4qmdU3a95917dy2lmlf+hmn2fiHnR4Tk7NHu4/EuN9k6EZ2YzaPe1sF80Ln3NTlW+fvgaZldfp5rV7ZqOiiWPLZDxBzmpl/Vaj3b/i4z9oPpOzgm1GNyRZvXevXp8tIwz186UceRWle+e/7LJ2HJA3U/Hd6om5xh/1ZUjvvr2sspKjouJfTnvVc0zztfGKP6ht4wT9re9+FhnJurUZlOb6wMdnhtiVb+vq9Yuk/HFnyXOrD2nulF2a/p8x4SoBdhKGtT9E/mO+RYTuxS2fBAREZGhWPkgIiIiQ7HbpYMsX5TJ+OydaqGZ9IkzZLz50VdkvGuMaqr+Ve+xMq4a4aYCmlSTakVHhF+Qw3PFdWrk+JXvHFHXuLlM2j1mdr04UPNMqYx+tXe8wzUpD+2TsRkX8rlY31+rRaQG5KlZWAnph11+rfXH1IJgxz9Wo/W7b1fNzEFrvtJcoY4no8Tpa2rfw8OPDpNxulU1Jy87cwXIdd89pn6/tV2Jl5L4rIo5k6h1zTa1p9TT037n8NyLb6hFx67VfNT9rVrNdplXdKuMkwvU/i8BNrXQZfTfT8l4TMI6GU9ar77fpe4rM2LLBxERERmKlQ8iIiIyFLtddKRteotZqOK6R1SHQahFtbu91Xu1jG+5faY6Z8VmN5XQO5xs7ipjdy/Epu1qKX/2Ghnvuk3NBvj4rNqn50h+X4frw067Z0txIyTN0W9kfE/ov/BU6MjjTo8/sT5LxsnwzMJz3sI+Si0EN2/QyjbP/9k2tTdI1xLOcGmPoE8cuz4eSxrc5jWX+j2uuU1d+3+Jq2TcKFS7Qch+x25rb8GWDyIiIjIUKx9ERERkKHa7dJB9RKqM99ylFusZmLpfxtquFq1Fp1STaOgq7xml7G6zv7hLxsmamSZ60TZFH8s9J+Odg1RXy0+/nSjjLj9XC8CFwXu7WXxFr1Wce3G55heovUEGBjp/32YfHSnjiHtPy9gbZm/5uqYQ1T5wqcUOkwpUl6e7ZwTqiS0fREREZChWPoiIiMhQ7Ha5TJZBatGp7zTbIr81fKmMRwY3oC31Qi20tOlUknrCfrSDJfQymm3U/S6qA78y4u8yzkcy9HBgrtpL5v3f/EnGyYEqlzdsmSTjuNt36PJ9iTzp+iDnzfZaxUtukHH0ac/sn0TOhS3TdPO+5LlyuINLLR95eXlIT09HWFgYoqOjMWHCBJSXlzucU1dXh+zsbHTv3h1du3ZFVlYWbDbbJV6R9LJP7MIWUYj1YiWKxIf4RnyJWlHzo/Mefvhh5sZAzIt5MTfmxdz4PpcqH0VFRcjOzsamTZvw6aeforGxEWPHjkVtba08Z9asWfjwww+xfPlyFBUV4ciRI7jjjjt0Lzg5qsRxxKMP0jEGN+BG2GHHVnyOZuE4BGnNmjXMjYGYF/NibsyLufF9LnW7rFmzxuFxQUEBoqOjUVpaipEjR6Kqqgpvv/023n33Xdx0000AgCVLluDqq6/Gpk2bMHToUP1K7iYBSb1kvGdynIz/MFFthZzV9YRLr/mYbZCMi15R70G3pfot8nS95UaHxwNEOjbgQ1TjNLrhJ2g6v6/G/PnzzZEbzcD7i7eWHhVyUsYzC9Jk3GeJOi+wQv0XZBv1ExlHTVTbsM9ILJTx+FA1a+aD2hgZ/+bbn8u4x5tdLrv4l8vr8mJS/hb1f9LpZLVleezH7X9NX83NofdUF3GgpazN83t+pj7PzDLDxVdz46qae7Q/h/4z/zypQwNOq6paNr2JiooCAJSWlqKxsRGZmZnynJSUFCQmJqK42Pkf2vr6elRXVzt8UcdduDkD0TKmoQaVAIDRo0fLc5gb4+mRF4C5cQfeM+bF3Piedlc+7HY7Zs6cieHDh2PgwJaadkVFBYKCghAZGelwbkxMDCoqKpy+Tl5eHiIiIuRXQkKC0/Po8gkh8B3KEIHu6GppWRq8AfUAwNx4kF55AZgbvfGeMS/mxje1e7ZLdnY2tm3bho0bN3aoAHPmzEFubq58XF1dbcgvRUDvRBlXpfWU8cS5qmtpauQ/XXrNh4+qJrLi11RXS1SBWre/m12/rpZL2YWtOINqDMLoDr2Op3ITbFG/ljt/9oaMN96oFnHbXR8r48kR+9t8zYeOqGbcNV+myrjfQ8YtGqZXXgDP5cZTmoWma84NCwR4+z2jXTjv5dS/yVg7w6XKrrZqT/94poxTDph7Zpe356Yjqq703dUw2lX5yMnJwerVq7FhwwbEx8fL47GxsWhoaEBlZaVDjdRmsyE2NtbJKwFWqxVWq7U9xSAndomtOIGjGITRCLaoTdOC0PIeV1ZWIjw8XB5nboyhZ14A5kZPvGfMi7nxXS5Vq4QQyMnJwYoVK7Bu3TokJSU5PJ+WlobAwEAUFqqBfuXl5Th48CAyMjIufjnSkRACu8RWHMdhpGEkQiyOgyfDEAmgZcbSBcyN+zEv5sXcmBdz4/tcavnIzs7Gu+++i1WrViEsLEz2rUVERCAkJAQRERG4//77kZubi6ioKISHh2PGjBnIyMjwmdHHZlWOrajAIVyHYfBHIOpFSxNrAALhb/FHAFpmCDz++OOIj49nbgzCvJgXc2NezI3vc6ny8frrrwNwHGEMtExx+u1vfwsAWLBgAfz8/JCVlYX6+nqMGzcOr732mi6FdVVAT9X8dmqxY815WpKqMd8b5trCNDmHR8j469dTZdzjvW0yjqpx/9gOrR/QsvlZKYocjvfHIMSht3w8btw4U+Qm5rNjMn70Qcf/VJ6Ldf7eaVeQHRG83+k5W+tVY969RVNknDxZTVPrZ+DmcN6WF29wNv2sLq/jS7mpi1Ir9Y4IrtU84y+jT86qcW7JU76SseNEd3Pwpdx0xBVF6nc9MEflstEH9lZ0qfIhRNs/cXBwMPLz85Gfn9/uQpHrMi13XtZ5L730Et566y03l4YuYF7Mi7kxL+bG9/nuUFoiIiIyJZ/YWK5hnJrW2jDrlIwf6/uRjMeG1MJVtuZzMh75wcMyTnlil4yjKlUXgRmbL82q+bs9Mt59V2+H5/rPmCHjHXcvavO1Uj6aLuOrXlPNlMlbfWtFwM5Mu8IpUWdh+aJMxgXV0TK+N+ywjM8OUEtFBB1SKzybHe9oIiIiMhQrH0RERGQon+h22T9B1aG+u2b5ZV2TX9lHxq8UjZWxpdki45R5+2Tcz7ZZxmbZfMlXNO3d7/C47yz1+NZZ6W1enww1ct8HBoHTefVr1YaBzans1GxNeJlaUnzGDzfJ+I2EImenkxda8KYahHvv7Fdk3PPJ72V8svJadcGmfxtSrvZiywcREREZipUPIiIiMpRPdLskT1Mbt90yLc3167HF6XF2rxB5TuyCL2V884IbZHwlyjxQGnNr2ndAxj9oFvi8Ba5/HpI5XfHXchlPnHCLjP/Rd7WMRz11r4yj7ouQcXNllZtL5zq2fBAREZGhWPkgIiIiQ/lEtwsREZEvaz5xUsYNWd1lfPVLD8p4Z+abMr415X51sQlnvrDlg4iIiAzFygcREREZit0uREREXkTbBdNvkopvhXZRRvN1tWix5YOIiIgMZbqWDyFaFshuQiPXytZRExoBqPe3PZgb/emRF+31zI1+eM+YF3NjTq7kxXSVj5qaGgDARnzk4ZL4ppqaGkRERLR94iWuBZgbd+hIXi5cDzA37sB7xryYG3O6nLxYREf/5dKZ3W7HkSNHIIRAYmIiDh06hPDwcE8XyxDV1dVISEhwy88shEBNTQ3i4uLg59e+3ja73Y7y8nL079+/U+UFcF9u9MgL0Hlz4w33DD/PzJsb3jOey4vpWj78/PwQHx+P6upqAEB4eHin+aW4wF0/c0f+swZacnPFFVcA6Jx5Adzzc3c0LwBzY+Z7hp9n5s0N7xnP5YUDTomIiMhQrHwQERGRoUxb+bBarXj66adhtVo9XRTDeMPP7A1ldAdv+Lm9oYx685af2VvKqSdv+Jm9oYx6M8vPbLoBp0REROTbTNvyQURERL6JlQ8iIiIyFCsfREREZChWPoiIiMhQpqx85Ofno3fv3ggODsaQIUOwZcsWTxdJN3l5eUhPT0dYWBiio6MxYcIElJeXO5xTV1eH7OxsdO/eHV27dkVWVhZsNpuHSuyIuWFujMa8mBdzY16mz40wmWXLlomgoCCxePFisX37dvHAAw+IyMhIYbPZPF00XYwbN04sWbJEbNu2TZSVlYmbb75ZJCYmijNnzshzpk6dKhISEkRhYaEoKSkRQ4cOFcOGDfNgqVswN8yNJzAv5sXcmJfZc2O6ysfgwYNFdna2fNzc3Czi4uJEXl6eB0vlPseOHRMARFFRkRBCiMrKShEYGCiWL18uz9m5c6cAIIqLiz1VTCEEc8PcmAPzYl7MjXmZLTem6nZpaGhAaWkpMjMz5TE/Pz9kZmaiuLjYgyVzn6qqKgBAVFQUAKC0tBSNjY0O70FKSgoSExM9+h4wN8yNWTAv5sXcmJfZcmOqyseJEyfQ3NyMmJgYh+MxMTGoqKjwUKncx263Y+bMmRg+fDgGDhwIAKioqEBQUBAiIyMdzvX0e8DcMDdmwLyYF3NjXmbMjel2te1MsrOzsW3bNmzcuNHTRaGLMDfmxLyYF3NjXmbMjalaPnr06AF/f/8fjba12WyIjY31UKncIycnB6tXr8b69esRHx8vj8fGxqKhoQGVlZUO53v6PWBumBtPY17Mi7kxL7PmxlSVj6CgIKSlpaGwsFAes9vtKCwsREZGhgdLph8hBHJycrBixQqsW7cOSUlJDs+npaUhMDDQ4T0oLy/HwYMHPfoeMDfMjacwL+bF3JiX6XPj9iGtLlq2bJmwWq2ioKBA7NixQ0yZMkVERkaKiooKTxdNF9OmTRMRERHis88+E0ePHpVfZ8+eledMnTpVJCYminXr1omSkhKRkZEhMjIyPFjqFswNc+MJzIt5MTfmZfbcmK7yIYQQixYtEomJiSIoKEgMHjxYbNq0ydNF0g0Ap19LliyR55w7d05Mnz5ddOvWTYSGhorbb79dHD161HOF1mBumBujMS/mxdyYl9lzYzlfSCIiIiJDmGrMBxEREfk+Vj6IiIjIUKx8EBERkaFY+SAiIiJDsfJBREREhmLlg4iIiAzFygcREREZipUPIiIiMhQrH0RERGQoVj6IiIjIUKx8EBERkaFY+SAiIiJD/T+eO52BZqa0/wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "encoder = Model(input_img,encoded)\n",
        "\n",
        "input_rep = Input(shape=(4,4,8))\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_rep)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "decoder = Model(input_rep,decoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "Qc1cOLt4lrjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ],
      "metadata": {
        "id": "cOHs5ij8l2Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=25,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBDhJm6Hl9Hx",
        "outputId": "715a60b2-f4a0-4287-cf50-2b61a32d5723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 152ms/step - loss: 0.3051 - val_loss: 0.1485\n",
            "Epoch 2/25\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 159ms/step - loss: 0.1425 - val_loss: 0.1279\n",
            "Epoch 3/25\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 150ms/step - loss: 0.1266 - val_loss: 0.1188\n",
            "Epoch 4/25\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 159ms/step - loss: 0.1186 - val_loss: 0.1142\n",
            "Epoch 5/25\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 166ms/step - loss: 0.1142 - val_loss: 0.1105\n",
            "Epoch 6/25\n",
            "\u001b[1m 66/469\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 166ms/step - loss: 0.1100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = autoencoder.predict(x_test[0:5])\n",
        "plotn(5,x_test)\n",
        "plotn(5,y_test)"
      ],
      "metadata": {
        "id": "OLgIMJ_JmBlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Task 1**: Try to train autoencoder with very small latent vector size, eg. 2, and plot the dots corresponding to different digits. *Hint: Use fully-connected dense layer after the convoluitonal part to reduce the vector size to the required value.*\n",
        "\n",
        "> **Task 2**: Starting from different digits, obtain their latent space representations, and see what effect adding some noise to the latent space has on the resulting digits.\n",
        "\n",
        "> **Task 3**: How and why is BCELoss used in the training of autoencoders, particularly when dealing with binary or probabilistic outputs?\n",
        "\n",
        "> **Task 4**: In the context of a convolutional autoencoder (conv2d AE), how do the downsampling and upsampling procedures work, and why are they important for the reconstruction process?"
      ],
      "metadata": {
        "id": "UEs_3AW1mne3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Denoising\n",
        "\n",
        "Autoencoders can be effectively used to remove noise from images. In order to train denoiser, we will start with noise-free images, and add artificial noise to them. Then, we will feed autoencoder with noisy images as input, and noise-free images as output.\n",
        "\n",
        "Let's see how this works for MNIST:"
      ],
      "metadata": {
        "id": "_GKBxresmyZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noisify(data):\n",
        "  return np.clip(data+np.random.normal(loc=0.5,scale=0.5,size=data.shape),0.,1.)\n",
        "\n",
        "x_train_noise = noisify(x_train)\n",
        "x_test_noise = noisify(x_test)\n",
        "\n",
        "plotn(5,x_train_noise)"
      ],
      "metadata": {
        "id": "6HCRJg8smmRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train_noise, x_train,\n",
        "                epochs=25,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noise, x_test))"
      ],
      "metadata": {
        "id": "sCLrpDkhm9LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = autoencoder.predict(x_test_noise[0:5])\n",
        "plotn(5,x_test_noise)\n",
        "plotn(5,y_test)"
      ],
      "metadata": {
        "id": "OaVZWAnsnA38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Exercise:** See how denoiser trained on MNIST digits works for different images. As an example, you can take [Fashion MNIST](https://keras.io/api/datasets/fashion_mnist/) dataset, which has the same image size. Note that denoiser works well only on the same image type that it was trained on (i.e. for the same probability distribution of input data)."
      ],
      "metadata": {
        "id": "Mr9b_kMNnIZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Super-resolution\n",
        "\n",
        "Similarly to denoiser, we can train autoencoders to increase the resolution of the image. To train super-resolution network, we will start with high-resolution images, and automatically downscale them to produce network inputs. We will then feed autoencoder with small images as inputs and high-res images as outputs.\n",
        "\n",
        "Let's downscale MNIST to 14x14:"
      ],
      "metadata": {
        "id": "_YY6apQGnMcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_lr = tf.keras.layers.AveragePooling2D()(x_train).numpy()\n",
        "x_test_lr = tf.keras.layers.AveragePooling2D()(x_test).numpy()\n",
        "plotn(5,x_train_lr)"
      ],
      "metadata": {
        "id": "Ls628FCGnPwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = Input(shape=(14, 14, 1))\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "encoder = Model(input_img,encoded)\n",
        "\n",
        "input_rep = Input(shape=(4,4,8))\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_rep)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "decoder = Model(input_rep,decoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "cLHxNuUvnTam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train_lr, x_train,\n",
        "                epochs=25,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_lr, x_test))"
      ],
      "metadata": {
        "id": "oEoz6fgJnWs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_lr = autoencoder.predict(x_test_lr[0:5])\n",
        "plotn(5,x_test_lr)\n",
        "plotn(5,y_test_lr)"
      ],
      "metadata": {
        "id": "7KHD0WoknZJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Exercise**: Try to train super-resolution network on [CIFAR-10](https://keras.io/api/datasets/cifar10/) for 2x and 4x upscaling. Use noise as input to 4x upscaling model and observe the result."
      ],
      "metadata": {
        "id": "3usg_Phvnbvx"
      }
    }
  ]
}